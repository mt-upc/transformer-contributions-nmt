{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "torch.cuda.set_device(5)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from wrappers.transformer_wrapper import FairseqTransformerHub\n",
    "from wrappers.multilingual_transformer_wrapper import FairseqMultilingualTransformerHub\n",
    "from alignment.aer import aer\n",
    "import itertools\n",
    "\n",
    "import alignment.align as align\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.setLevel('WARNING')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'bilingual'# bilingual/multilingual\n",
    "model_size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'bilingual':\n",
    "    # Bilingual paths\n",
    "    europarl_dir = Path(os.environ['EUROPARL_DATA_DIR'])\n",
    "    ckpt_dir = Path(os.environ['EUROPARL_CKPT_DIR'])\n",
    "    # Choose model\n",
    "    model_type = 'baseline'\n",
    "    seed = 9819 # 2253  2453  5498  924  9819\n",
    "    model_name = f\"{model_type}/{seed}\"\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        src = \"de\",\n",
    "        tgt = \"en\",\n",
    "        tokenizer = \"bpe\",\n",
    "        test_set_dir = Path(os.environ['EUROPARL_DATA_DIR']) / \"processed_data/\",\n",
    "        model_name_save = model_name.replace('/','_'),\n",
    "        pre_layer_norm = False,\n",
    "        num_layers = 6\n",
    "        )\n",
    "\n",
    "elif model == 'multilingual':\n",
    "    # Multilingual paths\n",
    "    ckpt_dir = Path(os.environ['M2M_CKPT_DIR'])\n",
    "    europarl_dir = Path(\"./data/de-en\")\n",
    "    model_size = 'small' # small (412M) /big (1.2B)\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        src = \"de\",\n",
    "        tgt = \"en\",\n",
    "        tokenizer = \"spm\",\n",
    "        test_set_dir = Path(\"./data/de-en\").as_posix(),\n",
    "        model_name_save = f'm2m100_{model_size}',\n",
    "        pre_layer_norm = True,\n",
    "        num_layers = 12\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt_dir = Path(os.environ['IWSLT14_CKPT_DIR'])\n",
    "\n",
    "lang_flores_dict = {'en': 'eng', 'es': 'spa', 'zu': 'zul',\n",
    "                    'de': 'deu', 'yo': 'yor', 'ms': 'msa',\n",
    "                    'fr': 'fra', 'xh': 'xho'}\n",
    "source_lang = 'de'\n",
    "target_lang = 'en'\n",
    "if model == 'bilingual':\n",
    "    hub = FairseqTransformerHub.from_pretrained(\n",
    "        ckpt_dir / f\"{model_type}/{seed}\",\n",
    "        checkpoint_file=f\"checkpoint_best.pt\",\n",
    "        data_name_or_path=(europarl_dir / \"processed_data/fairseq_preprocessed_data\").as_posix(), # processed data\n",
    "    )\n",
    "\n",
    "elif model == 'multilingual':\n",
    "    # Checkpoint names\n",
    "    if model_size=='big':\n",
    "        checkpoint_file = '1.2B_last_checkpoint.pt'\n",
    "    else:\n",
    "        checkpoint_file = '418M_last_checkpoint.pt'\n",
    "    data_name_or_path='.'\n",
    "    hub = FairseqMultilingualTransformerHub.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        checkpoint_file=checkpoint_file,\n",
    "        data_name_or_path=data_name_or_path,\n",
    "        source_lang= args.src,\n",
    "        target_lang= args.tgt,\n",
    "        lang_pairs =f'{source_lang}-{target_lang}',\n",
    "        fixed_dictionary=f'{ckpt_dir}/model_dict.128k.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_list = ['alti', 'decoder.encoder_attn', 'alti_enc_cross_attn','attn_w']\n",
    "aer_obt = aer(args, mode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "contrib_type = 'l1'\n",
    "aer_obt.extract_contribution_matrix(hub, args.model_name_save, contrib_type,\n",
    "                                pre_layer_norm=args.pre_layer_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alti\n",
      "decoder.encoder_attn\n",
      "alti_enc_cross_attn\n",
      "attn_w\n",
      "alti\n",
      "decoder.encoder_attn\n",
      "alti_enc_cross_attn\n",
      "attn_w\n"
     ]
    }
   ],
   "source": [
    "aer_obt.extract_alignments(final_punc_mark=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWI:\n",
      "\n",
      "Mode: alti\n",
      "[0.5973234794766803, 0.4379306901028662, 0.4624488165384999, 0.5834415260161789, 0.6851093578348147, 0.700838909417757]\n",
      "\n",
      "Mode: decoder.encoder_attn\n",
      "[0.47103765105363027, 0.2602533927587761, 0.3611249875112399, 0.8550934048921457, 0.830720063916908, 0.8438843884388438]\n",
      "\n",
      "Mode: alti_enc_cross_attn\n",
      "[0.5973234794766803, 0.41980425446919, 0.4998002596624388, 0.8016479400749064, 0.8072505742534705, 0.8353140916808149]\n",
      "\n",
      "Mode: attn_w\n",
      "[0.6170462894930198, 0.2819765747916788, 0.46417410080118193, 0.9033638068448195, 0.9321608040201005, 0.9402746494223643]\n",
      "\n",
      "AWO:\n",
      "\n",
      "Mode: alti\n",
      "[0.7944172575651653, 0.7859782283032059, 0.7295016478577849, 0.6367721961450115, 0.5687106761210426, 0.5575751523020074]\n",
      "\n",
      "Mode: decoder.encoder_attn\n",
      "[0.8521921502047338, 0.8555378008588834, 0.6951499999999999, 0.47756297006713444, 0.38919404773794064, 0.5158]\n",
      "\n",
      "Mode: alti_enc_cross_attn\n",
      "[0.7944172575651653, 0.7869269949066213, 0.6630380505343054, 0.5599720363527414, 0.5099870168780585, 0.5739039248976331]\n",
      "\n",
      "Mode: attn_w\n",
      "[0.9181712288875946, 0.8862258322846356, 0.6764975900849208, 0.49905611807104855, 0.5047549821309589, 0.6424573088641679]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_list = ['alti', 'decoder.encoder_attn', 'alti_enc_cross_attn', 'attn_w']\n",
    "\n",
    "for setting in ['AWI', 'AWO']:\n",
    "    print(f'{setting}:\\n')\n",
    "    results = aer_obt.calculate_aer(setting)\n",
    "    for mode in mode_list:\n",
    "        print('Mode:', mode)\n",
    "        print(results[mode]['aer'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d90d8f5aa2056217711987bb6fa8dc20d62369a1e594ceedbb30cde0480a32e5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
